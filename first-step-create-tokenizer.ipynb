{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5349a436",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fcf32c",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b374fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from itertools import chain\n",
    "import pickle\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d9489",
   "metadata": {},
   "source": [
    "### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bc281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  of the Phoenician alphabet, the equivalent of the Hebrew Aleph, and itself from the Egyptian origin. Aleph was a consonant letter, with a guttural breath sound that was not an element of Greek articulation; and the Greeks took it to represent their vowel Alpha with the a sound, the Phoenician alphabet having no vowel symbols.\n",
      "\n",
      "The Last Line:  Designating, or pertaining to, a certain class of diseases. See Zymotic disease, below.\n"
     ]
    }
   ],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000\n",
    "filename = \"all5\"\n",
    "file = open(\"data/\" + filename + \".txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "l = 0\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    l = l + 1\n",
    "if (l > 5):\n",
    "    print(\"The First Line: \", lines[0])\n",
    "    print(\"The Last Line: \", lines[-1])\n",
    "else:\n",
    "    print(len(lines[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b572b1",
   "metadata": {},
   "source": [
    "### Remove all lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8c18dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of the Phoenician alphabet, the equivalent of the Hebrew Aleph, and itself from the Egyptian origin. Aleph was a consonant letter, with a guttural breath sound that was not an element of Greek articulation; and the Greeks took it to represent their vowel Alpha with the a sound, the Phoenician alphabet having no vowel symbols. is the name of a tone intermedia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2b733",
   "metadata": {},
   "source": [
    "### Remove all symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c42041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of the Phoenician alphabet the equivalent of the Hebrew Aleph and itself from the Egyptian origin Aleph was a consonant letter with a guttural breath sound that was not an element of Greek articulation and the Greeks took it to represent their vowel Alpha with the a sound the Phoenician alphabet having no vowel symbols is the name of a tone intermediate between and G An adjective commonly called the indefinite article and signifying one or any but less emphatically In each to or for each as twen'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "data = data.translate(translator)\n",
    "data = data.replace('  ', ' ').replace('  ', ' ')\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545f0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''.join(filter(lambda x: not x.isdigit(), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e49e7f",
   "metadata": {},
   "source": [
    "### Extract unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7ff140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Abstruse chalaza collet hemlock grieved corrode rehearsed hale preraphaelite Pneumatometer lotion Mississipi beets bespangle Carbus catechu Last lasting Continuance bigger serpula receptibility Series possessive Quart hardening Myrtaceae infects clandestine disappointments Slates misshapen evenness majesty qualifying hydroid HgOSO peptonized knocked granulate cogitative Brittany Bill impropriates Acnode gyrating Flamen melodies flooring probative ruffle mussels acquiring oust cubebs Presbyterian'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = set(chain(*(line.split() for line in data.split() if line)))\n",
    "print(len(z))\n",
    "\n",
    "unique_words = ' '.join(z)\n",
    "\n",
    "unique_words[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92179333",
   "metadata": {},
   "source": [
    "### Save Tokenizer and cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72b82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/\" + filename + \"-cleaned.txt\", 'w', encoding='utf-8') as output:\n",
    "    output.write(data)\n",
    "\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([unique_words])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer/tokenizer-'+filename+'.pkl', 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a60de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3933], [1202], [], [1129], [3229], [2471], [], [2332], [3229], [3933]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(data[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e7de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
