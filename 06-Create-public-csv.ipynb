{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c894e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tf.__version__\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9143373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"qa-10M\"\n",
    "tokenizer = pickle.load(open('tokenizer/tokenizer-' + filename + '.pkl', 'rb'))\n",
    "\n",
    "model = load_model('nextword-'+filename+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de461a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    #print(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = np.array(sequence)\n",
    "    res = []\n",
    "    preds = model.predict(sequence)\n",
    "    preds1= np.flip(np.argsort(preds,axis=1))\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "    for i in range(10):\n",
    "        res.append((preds1[0][i]))\n",
    "    #print(len(res))\n",
    "    predicted_word = \"\"\n",
    "    predicted_words = []\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value in res:\n",
    "            predicted_words.append(key)\n",
    "            #print(key)\n",
    "            #break\n",
    "\n",
    "    #print(len(predicted_word))\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50067ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'thanks', 'tv', 'its', 'floppy', 'eminem', 'say', 'kick', 'hasn', 'gps']\n",
      "27783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Obviously\";\n",
    "export = Predict_Next_Words(model, tokenizer, text)\n",
    "print(export)\n",
    "\n",
    "print(len(tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5ace62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\twow\tsomething\ttv\tiphone\tas\toooh\taudi\tconstituents\tvr\tterry\n",
      "And\tthanks\ttv\tha\tmp\teminem\toooh\tmust\ttoy\tpoliticians\thasn\n",
      "Boy\tthanks\ttv\tsure\tmp\toooh\tinto\tmust\ttoy\tpoliticians\thasn\n",
      "Book\tthanks\ttv\tright\tempire\tfor\thorrible\tsundays\thasn\tyoure\tblatant\n",
      "Call\twow\tthanks\ttv\tha\tmp\tamazing\teminem\toooh\tpoliticians\thasn\n",
      "Car\twow\tthanks\tnike\ttv\tkick\tpoliticians\thasn\taway\tgps\tamazons\n",
      "Chair\twow\tthanks\ttv\tha\tmp\teminem\toooh\tperhaps\tpoliticians\thasn\n",
      "Children\twow\tthanks\tnike\ttv\twhat\tbut\teve\tbanned\thasn\tangeles\n",
      "City\tthanks\ttv\tha\tmp\tonly\teminem\toooh\tpoliticians\thasn\taway\n",
      "Dog\twow\tnike\ttv\tha\tmp\tonly\tstates\toooh\tpoliticians\thasn\n",
      "Door\tthanks\ttv\tits\tmp\tonly\teminem\toooh\tpoliticians\thasn\taway\n",
      "Enemy\twow\tthanks\ttv\tha\tspacex\teminem\toooh\tperhaps\tpoliticians\thasn\n",
      "End\twow\tthanks\ttv\tmp\tamazing\tonly\teminem\toooh\tpoliticians\thasn\n",
      "Enough\tthanks\ttv\tmp\teminem\toooh\tinto\tmust\ttoy\tpoliticians\thasn\n",
      "Eat\tthanks\ttv\tits\tonly\teminem\toooh\tmust\tsay\thasn\tnetherlands\n",
      "Friend\twow\tthanks\ttv\tha\tmp\teminem\toooh\ttoy\tpoliticians\thasn\n",
      "Father\tthanks\ttv\tmp\tamazing\tspacex\tonly\teminem\toooh\tpoliticians\thasn\n",
      "Go\twow\tthanks\tnike\ttv\tright\tway\tunbelievable\thasn\twilderness\tlady\n",
      "Good\tthanks\ttv\tmp\teminem\toooh\tmust\ttoy\tpatrons\tpoliticians\tcider\n",
      "Girl\twow\tthanks\ttv\tspacex\teminem\toooh\tsundays\ttesla\tpoliticians\thasn\n",
      "Food\twow\tthanks\ttv\tmp\tamazing\tonly\teminem\toooh\tpoliticians\thasn\n",
      "Hear\ttv\tmp\teminem\toooh\tinto\tmust\ttoy\tpoliticians\thasn\tcider\n",
      "House\tthanks\tnike\tcombined\tinteresting\thorrible\tkick\twaht\tshaky\tons\treevaluate\n",
      "Inside\twow\tthanks\ttv\teminem\toooh\tmust\tsay\tpoliticians\thasn\tgps\n",
      "Laugh\twow\tthanks\ttv\tha\tmp\tonly\teminem\toooh\tpoliticians\thasn\n",
      "Listen\twow\ttv\tyes\tsure\teminem\tsundays\tpoliticians\thasn\tcider\tnetherlands\n",
      "Man\tthanks\ttv\tsure\tmp\teminem\toooh\ttoy\tpoliticians\thasn\tcider\n",
      "Name\tthanks\ttv\tha\tmp\teminem\toooh\tsundays\tmust\tpoliticians\thasn\n",
      "Never\twow\tthanks\ttv\tabba\tkart\tthatâ€™s\tweirdly\tcamaro\tvadar\tweather\n",
      "Next\tthanks\ttv\tits\tmp\tonly\teminem\toooh\tmust\tpoliticians\thasn\n",
      "New\tnike\tsuperbowl\tway\tlight\twaht\tdenmark\tmanipulated\ttreasonous\toverbooked\tkorn\n",
      "Noise\twow\tthanks\ttv\tmp\tspacex\teminem\toooh\ttoy\tpoliticians\thasn\n",
      "Often\tlebron\twow\tthanks\tmp\tinteresting\tspacex\tperhaps\tpoliticians\thasn\tjackass\n",
      "Pair\twow\tthanks\ttv\tha\teminem\toooh\tnextflix\tkick\tpoliticians\thasn\n",
      "Pick\twow\tthanks\ttv\teminem\tsundays\ttoy\tpoliticians\thasn\tcider\tnetherlands\n",
      "Play\tthanks\ttv\tits\toooh\tinto\tsundays\tmust\tsheep\thasn\twaters\n",
      "Room\tyoutube\ttv\thow\timax\teverything\tmp\tfloppy\tprobably\tonly\tpoliticians\n",
      "See\twow\tthanks\ttv\tamazing\teminem\toooh\tsundays\ttoy\tpoliticians\thasn\n",
      "Sell\twow\tthanks\ttv\tmp\tonly\teminem\toooh\tpoliticians\thasn\taway\n",
      "Sit\twow\tthanks\ttv\twho\tspacex\tmb\teminem\tsundays\ttesla\ttoy\n",
      "Speak\twow\ttv\tsure\tspacex\tmb\tkalakaua\tmust\ttoy\tcider\tnetherlands\n",
      "Smile\twow\tthanks\twho\tha\tmp\tknew\tstarship\ttoy\tperhaps\tpoliticians\n",
      "Sister\tthanks\ttv\tmp\tamazing\tonly\teminem\toooh\tmust\tpoliticians\thasn\n",
      "Think\tthanks\tinteresting\tspacex\ttoy\tperhaps\tnextflix\tunbelievable\tpoliticians\tdepends\tcmt\n",
      "Then\tthanks\ttv\tits\tmp\tonly\teminem\toooh\tmust\tpoliticians\thasn\n",
      "Walk\twow\tthanks\ttv\tha\tspacex\teminem\ttoy\tnextflix\tpoliticians\thasn\n",
      "Water\twow\tthanks\ttv\tits\teminem\toooh\tsay\thasn\tgps\tapron\n",
      "Work\twow\ttv\tha\tright\tonly\teminem\tstates\toooh\tsundays\thasn\n",
      "Write\twow\tthanks\ttv\tha\teminem\toooh\tsundays\ttoy\tpoliticians\thasn\n",
      "Woman\twow\ttv\twho\timax\tha\tgallons\tone\thot\ttoy\tpoliticians\n"
     ]
    }
   ],
   "source": [
    "for key, value in tokenizer.word_index.items():\n",
    "    export = Predict_Next_Words(model, tokenizer, key)\n",
    "    print(str(value) + \"\\t\" + key + \"\\t\" + \"\\t\".join(export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc80002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4264785e-16 2.6515091e-19 8.4464776e-16 ... 5.4372945e-16\n",
      "  5.0407757e-16 6.5618330e-16]]\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences([\"Hello\"])\n",
    "sequence = np.array(sequence)\n",
    "print(model.predict(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
