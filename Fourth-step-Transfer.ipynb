{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51f04ac",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395cde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tf.__version__\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import string\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714cc022",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"qa-1M\"\n",
    "fname = \"lazar\"\n",
    "tokenizer = pickle.load(open('tokenizer/tokenizer-' + filename + '.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04bb18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:   No, like I said, I'm not going to really concern myself with that. Not till later.\n",
      "\n",
      "The Last Line:   From his mom...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" No, like I said, I'm not going to really concern myself with that. Not till later.  I wasn't told what I was working on...  At that point... no.  Yeah.  The best thing for me to do is to pull out...  I have these calendars... they're big wall calendars where I write what happened every day on it... and I've had them since like 1980... so I have exact dates.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"data/\" + fname + \".txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "l = 0\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    l = l + 1\n",
    "if (l > 5):\n",
    "    print(\"The First Line: \", lines[0])\n",
    "    print(\"The Last Line: \", lines[-1])\n",
    "else:\n",
    "    print(len(lines[0]))\n",
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0555f71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1607, 35, 867, 1551, 10, 1018, 54, 60, 9103, 8837]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c420a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e552f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineData():\n",
    "    sequences = []\n",
    "    for i in range(1, int(len(sequence_data))):\n",
    "        words = sequence_data[i-1:i+1]\n",
    "        sequences.append(words)\n",
    "\n",
    "    #print(\"The Length of sequences are: \", len(sequences))\n",
    "    sequences = np.array(sequences)\n",
    "    #sequences[:10]\n",
    "\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in sequences:\n",
    "        X.append(i[0])\n",
    "        y.append(i[1])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    #y[:5]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee3c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('nextword-'+filename+'.h5')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword-\"+filename+\".h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword'+filename\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74dadb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [1607   35  867 1551   10]\n",
      "The responses are:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X,y = defineData()\n",
    "\n",
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc93898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9d2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1607   35  867 ...  257 1475  559] 36078\n"
     ]
    }
   ],
   "source": [
    "print(X , len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b0eb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    #print(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = np.array(sequence)\n",
    "    res = []\n",
    "    preds = model.predict(sequence)\n",
    "    preds1= np.flip(np.argsort(preds,axis=1))\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "    for i in range(10):\n",
    "        res.append((preds1[0][i]))\n",
    "    #print(len(res))\n",
    "    predicted_word = \"\"\n",
    "    predicted_words = []\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value in res:\n",
    "            predicted_words.append(key)\n",
    "            #print(key)\n",
    "            #break\n",
    "\n",
    "    #print(len(predicted_word))\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7e051b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'it', 'well', 'an', 'when', 'at', 'this', 'oh', 'she', 'cause']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Obviously\";\n",
    "export = Predict_Next_Words(model, tokenizer, text)\n",
    "print(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695dcdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\tthe\tthat\tit\tthere\tthis\ti\tthese\tjust\tof\tkinds\n",
      "And\tthe\tthat\tthey\tit\tthere\tso\tthen\ti\twe\the\n",
      "Boy\tand\tthat\tyou\twhen\ti\todd\tyeah\tprobably\tjoke\twierd\n",
      "Book\tand\tor\tthere\twas\tmore\ttogether\twhere\thad\twe\tover\n",
      "Call\tand\tthe\tthat\tit\tto\tbut\tfrom\thim\tthem\t4\n",
      "Car\tand\tthat\tin\tthey\tis\tto\twas\tback\tgo\ti\n",
      "Chair\tthat\tor\tout\tso\tdown\twhich\twith\tme\tfrom\tof\n",
      "Children\tand\tthe\tit\tlike\tso\ton\ti\tyeah\tfor\ta\n",
      "City\tand\tso\tmaterials\toh\tfrom\tof\tah\tmaintenance\ttruck\twierd\n",
      "Dog\tand\tthat\tis\tso\tas\twith\ti\tinstead\tstation\tthing\n",
      "Door\tand\tthe\tthey\tit\tto\tso\tbut\ton\ti\tyeah\n",
      "Enemy\tand\tthat\tso\ton\ti\tfrom\twho\tstation\tthing\ttotal\n",
      "End\tand\tthat\tor\tto\twell\ti\tafter\tof\tironically\tno\n",
      "Enough\tand\tthe\tit\tto\tbut\tat\ton\ti\tyeah\ta\n",
      "Eat\tand\tthat\tin\tus\twell\ti\tjust\tsee\tright\ta\n",
      "Friend\tthat\tit\tor\tout\tbut\tmore\tat\tgoing\twho\tof\n",
      "Father\ti\tyeah\tmuch\tedge\tessentially\treasons\twrote\tdied\tspeakers\tcompositions\n",
      "Go\tand\tout\tto\tthrough\tback\ton\tdown\tup\tahead\tinto\n",
      "Good\tand\tin\twell\tglad\tpoint\tfriend\timpression\tanalogy\tjob\tportion\n",
      "Girl\twhat\tand\tthat\tin\tyou\tso\twell\tbut\ton\tof\n",
      "Food\tthe\tin\tit\tout\tthere\tat\teither\tup\tright\ta\n",
      "Hear\tand\tthe\twas\twhen\thalf\tpeople\tany\tyeah\ta\tvarious\n",
      "House\tand\twell\teveryone\ther\tif\ti\tyeah\tjust\tof\tcause\n",
      "Inside\tand\tthe\tit\tlike\tthere\tbut\tinside\ti\ta\tof\n",
      "Laugh\tand\tthe\tso\tbut\tat\tup\twe\tfrom\tof\tcoming\n",
      "Listen\tand\tthe\tthat\tin\tit\ti\tyeah\toh\there\tcause\n",
      "Man\tand\tthe\tin\tat\ton\twith\tup\twe\tfor\tof\n",
      "Name\tand\tthat\tin\tthey\twell\tyour\tyeah\the\tunder\trank\n",
      "Never\tgot\tdid\tknew\theard\teven\thave\tsee\tseen\thappened\tsaw\n",
      "Next\tto\tmorning\ttime\ti\tokay\tfor\tof\tday\ttrip\toff\n",
      "New\tthat\tsort\tyork\ttype\tmexico\tcomes\tarmor\tsuccess\tjob\trays\n",
      "Noise\tand\tyou\tthere\twell\tyour\tother\tgoing\tyeah\tfrom\tno\n",
      "Often\tand\tthe\tthat\twas\tstar\twith\tfine\ti\tnow\tafter\n",
      "Pair\tand\tthat\tso\tbut\tbecause\ton\tfrom\twho\tof\taround\n",
      "Pick\tand\tto\tas\twith\tup\tdc\tof\tchanges\textinct\tforehead\n",
      "Play\tthe\tthat\tin\tout\ton\tthis\ta\tno\tinto\taround\n",
      "Room\tand\tthey\tso\tmet\tbut\twhich\ti\tlooking\there\tof\n",
      "See\twhat\tand\tthe\tthat\tit\tyou\ti\tme\thim\tthem\n",
      "Sell\tand\tthe\tthat\tit\tan\ta\tacross\tessentially\tyears\twritten\n",
      "Sit\tand\tthere\tat\tdown\ttogether\twith\tup\tfor\there\tinto\n",
      "Speak\tthe\tin\tlike\tthere\tto\tso\twas\ti\ta\tthing\n",
      "Smile\tand\tit\ti\twe\tright\ta\there\tagain\tkristen\tcause\n",
      "Sister\tand\tknew\tto\twas\tat\tup\ti\tokay\tcomes\tkristen\n",
      "Think\tthe\tthat\tthey\tit\tso\twas\tthis\ti\the\ta\n",
      "Then\tthe\tthey\tit\tyou\ton\ti\twe\the\ta\tagain\n",
      "Walk\tand\tthe\tbut\ton\tup\tunder\tpast\tdirectly\tinto\taround\n",
      "Water\tand\tthe\tare\tit\tall\tbut\ton\ti\tplease\ta\n",
      "Work\tand\tin\tout\tthere\tat\tbecause\ton\twith\ti\tnow\n",
      "Write\twhat\tand\thow\tto\tso\tas\tbut\tat\ton\tme\n",
      "Woman\tthe\tthat\tit\tthere\tto\tso\twith\ti\thad\tfor\n"
     ]
    }
   ],
   "source": [
    "strs = [\"All\",\"And\",\"Boy\",\"Book\",\"Call\",\"Car\",\"Chair\",\"Children\",\"City\",\"Dog\",\"Door\",\"Enemy\",\"End\",\"Enough\",\"Eat\",\"Friend\",\"Father\",\"Go\",\"Good\",\"Girl\",\"Food\",\"Hear\",\"House\",\"Inside\",\"Laugh\",\"Listen\",\"Man\",\"Name\",\"Never\",\"Next\",\"New\",\"Noise\",\"Often\",\"Pair\",\"Pick\",\"Play\",\"Room\",\"See\",\"Sell\",\"Sit\",\"Speak\",\"Smile\",\"Sister\",\"Think\",\"Then\",\"Walk\",\"Water\",\"Work\",\"Write\",\"Woman\",\"Yes\"]\n",
    "\n",
    "for i in range(50):\n",
    "    export = Predict_Next_Words(model, tokenizer, strs[i])\n",
    "    print(strs[i] + \"\\t\" + \"\\t\".join(export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe84e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
