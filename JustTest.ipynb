{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c894e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tf.__version__\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9143373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"qa-10M\"\n",
    "tokenizer = pickle.load(open('tokenizer/tokenizer-' + filename + '.pkl', 'rb'))\n",
    "\n",
    "model = load_model('nextword-'+filename+'.h5')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword-\"+filename+\".h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword'+filename\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de461a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    #print(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = np.array(sequence)\n",
    "    res = []\n",
    "    preds = model.predict(sequence)\n",
    "    preds1= np.flip(np.argsort(preds,axis=1))\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "    for i in range(10):\n",
    "        res.append((preds1[0][i]))\n",
    "    #print(len(res))\n",
    "    predicted_word = \"\"\n",
    "    predicted_words = []\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value in res:\n",
    "            predicted_words.append(key)\n",
    "            #print(key)\n",
    "            #break\n",
    "\n",
    "    #print(len(predicted_word))\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50067ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'that', 'and', 'it', 'to', 'for', 'is', 'of', 'i', 's']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Obviously\";\n",
    "export = Predict_Next_Words(model, tokenizer, text)\n",
    "print(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5ace62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\tthe\tthat\tthose\tare\tof\tday\ttime\tover\tkinds\tkids\n",
      "And\tthe\twow\tits\talso\tare\twe\tit\tthey\tnot\ti\n",
      "Boy\tthe\tthat\tand\the\tit\tto\tthey\tis\ti\ta\n",
      "Book\tand\twith\tof\tsongs\tby\tabout\tplanets\tkorea\tchanges\tt\n",
      "Call\tthe\tthat\tand\the\tit\this\tthey\tis\tdo\ti\n",
      "Car\tthe\tthat\tit\tto\ton\te\talbum\ta\tpianos\tcosts\n",
      "Chair\tany\told\tplay\thope\tchatting\tpoint\tlife\tdoing\tlook\tearth\n",
      "Children\tthe\tthat\tand\tor\tto\tin\twith\tof\ti\ta\n",
      "City\tand\tjupiter\tun\tweather\tproject\tservice\te\ti\tproblem\tpolitical\n",
      "Dog\twe\the\tplanet\tshow\tis\tnew\tname\tcentury\tlook\tvideo\n",
      "Door\tlike\thave\ton\tis\tin\tgames\twanted\tsystem\tpast\tgame\n",
      "Enemy\tthe\tthat\tand\tdid\tor\tit\tis\ti\ta\ts\n",
      "End\tthe\tto\ttoy\turanus\tof\tshards\tmidnight\tdust\trecorded\tages\n",
      "Enough\tto\tfor\tjudy\tapart\tn\tganymede\tprecipitate\tperspective\texists\ttaj\n",
      "Eat\tthe\tand\tare\tit\tto\ton\tfor\tin\ti\ta\n",
      "Friend\tto\tof\tyup\tlight\ti\tvenus\taway\tvideo\tkorea\tchanges\n",
      "Father\tthat\tand\tdid\the\thaha\twas\tthey\tis\tdo\ti\n",
      "Go\tany\tsomething\tplay\tprobably\tright\teven\tmade\tdoing\tlook\tlittle\n",
      "Good\thave\tbye\tway\ttalking\tcall\tmusic\town\ti\ttraining\tcause\n",
      "Girl\tand\thave\twhen\tat\tfor\twith\tof\taround\tfrom\tby\n",
      "Food\tlebron\tguitars\tchildren\tsource\tgirl\teffect\ttrain\tcha\tpc\treferred\n",
      "Hear\tthat\tand\tare\tit\tto\ton\tfor\tis\tin\ti\n",
      "House\tthat\tbut\tdid\twe\the\this\thaha\tis\tout\ti\n",
      "Inside\tthe\tyeah\tand\tbut\tjudge\tone\tthey\tis\ti\ta\n",
      "Laugh\twow\tthat\tand\tbut\the\tto\tas\tis\tin\ti\n",
      "Listen\tthat\tdid\twe\the\this\thaha\tis\tdo\tout\ti\n",
      "Man\tthat\tto\tso\tme\tin\tdo\tof\ti\thanks\tvisited\n",
      "Name\tthose\tall\this\thaha\tout\twhy\tnot\thad\tdoes\thim\n",
      "Never\tsomething\tjust\tbeen\tknew\tplayed\tset\tthought\tgoing\tvisited\tfigured\n",
      "Next\tyeah\tbut\tyears\tdid\tas\tone\tis\tlife\thim\ts\n",
      "New\tand\tjackson\thelium\tcountry\torleans\tphone\tbrother\tplanets\thand\tlength\n",
      "Noise\tthe\tthat\tand\thow\tyes\tto\tin\tof\ti\tfrom\n",
      "Often\tcustomer\trules\ttravel\tpop\tdancer\tvisited\tsuit\tassociated\tdancers\trequired\n",
      "Pair\twhat\tor\tto\ton\tfor\tis\tin\ti\tby\tthan\n",
      "Pick\tthat\tand\tbut\twhat\twe\the\this\tas\tis\ti\n",
      "Play\tya\tat\tfor\twith\tof\tyet\tgames\tby\taway\tvideo\n",
      "Room\tthat\tdid\tall\twe\the\thaha\tis\tdo\tout\ti\n",
      "See\tthe\tthat\the\tit\this\thaha\tthey\ti\ta\thours\n",
      "Sell\tand\twhat\tor\tit\tso\tat\tas\ton\tfor\ti\n",
      "Sit\tthose\tall\twe\this\thaha\tnow\thas\tout\twhy\thad\n",
      "Speak\tlol\tany\tor\tare\tquite\thas\tat\tmade\tthough\tlittle\n",
      "Smile\twow\tor\twe\tnow\tas\tout\tseems\twhere\tthough\tbeing\n",
      "Sister\twas\tfilms\ti\tplanets\tseasons\tmovie\tcha\tmake\tfind\that\n",
      "Think\tthe\tthat\the\tit\tso\tthey\tof\ti\tabout\tvenus\n",
      "Then\tlol\tits\tsome\thow\twhat\tall\twe\the\twhen\tright\n",
      "Walk\tthat\tand\thave\twe\twhen\tis\twith\tof\tfrom\tby\n",
      "Water\tyeah\tand\thow\tdid\tall\thas\twas\tis\ti\ts\n",
      "Work\tany\tplay\tstill\teven\tmade\thear\tdoing\tlook\tlittle\tkind\n",
      "Write\tbut\thave\tya\tto\ton\tfor\tin\tfrom\tby\tlargest\n",
      "Woman\tyeah\tand\thow\tdid\twe\thas\twas\tis\tthats\ts\n"
     ]
    }
   ],
   "source": [
    "strs = [\"All\",\"And\",\"Boy\",\"Book\",\"Call\",\"Car\",\"Chair\",\"Children\",\"City\",\"Dog\",\"Door\",\"Enemy\",\"End\",\"Enough\",\"Eat\",\"Friend\",\"Father\",\"Go\",\"Good\",\"Girl\",\"Food\",\"Hear\",\"House\",\"Inside\",\"Laugh\",\"Listen\",\"Man\",\"Name\",\"Never\",\"Next\",\"New\",\"Noise\",\"Often\",\"Pair\",\"Pick\",\"Play\",\"Room\",\"See\",\"Sell\",\"Sit\",\"Speak\",\"Smile\",\"Sister\",\"Think\",\"Then\",\"Walk\",\"Water\",\"Work\",\"Write\",\"Woman\",\"Yes\"]\n",
    "\n",
    "for i in range(50):\n",
    "    export = Predict_Next_Words(model, tokenizer, strs[i])\n",
    "    print(strs[i] + \"\\t\" + \"\\t\".join(export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc80002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
